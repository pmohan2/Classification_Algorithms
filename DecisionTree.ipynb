{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from pprint import pprint\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(ts_lb,answer):\n",
    "    TN = 0\n",
    "    TP = 0\n",
    "    FN = 0\n",
    "    FP = 0\n",
    "    for i,j in zip(ts_lb,answer):\n",
    "        if j==1 and i==1:\n",
    "            TP += 1\n",
    "        elif(j==1 and i==0):\n",
    "            FN += 1\n",
    "        elif(j==0 and i==1):\n",
    "            FP += 1\n",
    "        elif(j==0 and i==0):\n",
    "            TN += 1\n",
    "    Accuracy = (TP + TN)/(TP + FP + TN + FN)\n",
    "    Precision = TP/(TP + FP)\n",
    "    Recall = TP/(TP + FN)\n",
    "    f1_score = (2*Precision*Recall)/(Precision + Recall)\n",
    "    return Accuracy, Precision, Recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_leaf(df,counter, min_samples, max_depth):\n",
    "    unique_classes = np.unique(df)\n",
    "    if len(unique_classes) == 1 or len(df)<=min_samples or counter==max_depth:\n",
    "        labelcol = df\n",
    "        uniq_cls, cnt = np.unique(labelcol, return_counts=True)\n",
    "        classification = unique_classes[cnt.argmax()]\n",
    "        return classification\n",
    "    else:\n",
    "        return False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_imp_test(df, col_index):\n",
    "    df.reset_index(inplace = True, drop = True)\n",
    "    classes = df.iloc[:,-1]\n",
    "    feature = df.iloc[:,col_index]\n",
    "    if len(feature.unique()) == 2:\n",
    "        gini_imp = 0\n",
    "        for i in np.unique(feature):\n",
    "            idx = np.where(feature == i)\n",
    "            label = classes.loc[idx].values\n",
    "            a, b = np.unique(label, return_counts = True)\n",
    "            list1 = [(i/sum(b))**2 for i in b]\n",
    "            prob = 1 - sum(list1)\n",
    "            wt = len(idx[0]) / df.shape[0]\n",
    "            gini_imp += wt * prob\n",
    "        return gini_imp, i\n",
    "    else:\n",
    "        label = np.sort(feature.unique())[1:-1]\n",
    "        best_gini_imp = float('inf')\n",
    "        split_val = 0\n",
    "        for i in label:\n",
    "            idx1 = np.where(feature > i)\n",
    "            idx2 = np.where(feature <= i)\n",
    "            if len(idx1[0]) > 2 and len(idx2[0]) > 2:\n",
    "        \n",
    "                b1, b1cnt = np.unique(classes.loc[idx1].values, return_counts = True)\n",
    "                b2, b2cnt = np.unique(classes.loc[idx2].values, return_counts = True)\n",
    "                list1 = [(i/sum(b1cnt))**2 for i in b1cnt]\n",
    "                list2 = [(i/sum(b2cnt))**2 for i in b2cnt]\n",
    "                prob1 = 1 - sum(list1)\n",
    "                prob2 = 1 - sum(list2)\n",
    "                gini = ((sum(b1cnt)/df.shape[0])*prob1) + ((sum(b2cnt)/df.shape[0])*prob2) \n",
    "                if gini < best_gini_imp:\n",
    "                    best_gini_imp = gini\n",
    "                    split_val = i\n",
    "                else:\n",
    "                    continue \n",
    "        return best_gini_imp, split_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_node(df):\n",
    "    best_gini_imp = float('inf')\n",
    "    value = 0\n",
    "    col = 0\n",
    "    for i in range(df.iloc[:,:-1].shape[1]):\n",
    "        gini, val = gini_imp_test(df, i) \n",
    "        if gini < best_gini_imp:\n",
    "            best_gini_imp = gini\n",
    "            value = val\n",
    "            col = i\n",
    "    return col, value, best_gini_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df(df, col_index, split_val):\n",
    "    feature = df.iloc[:,col_index]\n",
    "    if feature.dtypes == object:\n",
    "        temp1 = df[df.iloc[:,col_index] == split_val]\n",
    "        temp2 = df[df.iloc[:,col_index] != split_val]\n",
    "        return temp1, temp2\n",
    "    elif feature.dtypes != object:\n",
    "        temp1 = df[df.iloc[:,col_index] <= split_val]\n",
    "        temp2 = df[df.iloc[:,col_index] > split_val]\n",
    "        temp1.reset_index(inplace = True, drop = True)\n",
    "        temp2.reset_index(inplace = True, drop = True)\n",
    "        return temp1, temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_purity(data):\n",
    "    \n",
    "    label_column = data[:, -1]\n",
    "    unique_classes = np.unique(label_column)\n",
    "\n",
    "    if len(unique_classes) == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_data(data):\n",
    "    \n",
    "    label_column = data[:, -1]\n",
    "    unique_classes, counts_unique_classes = np.unique(label_column, return_counts=True)\n",
    "\n",
    "    index = counts_unique_classes.argmax()\n",
    "    classification = unique_classes[index]\n",
    "    \n",
    "    return classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree(df, counter = 0, min_samples = 1, max_depth = 5):\n",
    "    if (check_purity(df.values)) or (len(df) < min_samples) or (counter == max_depth):\n",
    "        classification = classify_data(df.values)\n",
    "        \n",
    "        return classification\n",
    "      \n",
    "    else:\n",
    "        counter += 1\n",
    "        column, value, gini = best_node(df)\n",
    "        print(\"Depth \"+str(counter)+\" Impurity measure: \"+str(gini))\n",
    "        branch1, branch2 = split_df(df, column, value)\n",
    "        if len(branch1) == 0 or len(branch2) == 0:\n",
    "            classification = classify_data(df.values)\n",
    "            return classification\n",
    "        query = \"{} <= {}\".format(column, value)\n",
    "        branch = {query: []}\n",
    "\n",
    "        left_branch = decision_tree(branch1, counter)\n",
    "        right_branch = decision_tree(branch2, counter)\n",
    "\n",
    "        if left_branch == right_branch:\n",
    "            branch = left_branch\n",
    "        else:\n",
    "            branch[query].append(left_branch)\n",
    "            branch[query].append(right_branch)\n",
    "        return branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_data):\n",
    "    cls = []\n",
    "    for i in range(len(test_data)):\n",
    "        t = model\n",
    "        col,_,val = list(t.keys())[0].split()\n",
    "        col = int(col)\n",
    "        try:\n",
    "            val = float(val)\n",
    "        except:\n",
    "            val = str(val)\n",
    "        key = list(t.keys())[0]\n",
    "        key_val = t[key]\n",
    "        while True: \n",
    "            if test_data.iloc[i,col] <= val:\n",
    "                t = t[key][0]\n",
    "                if type(t) != dict:\n",
    "                    cls.append(t)\n",
    "                    break\n",
    "                else:\n",
    "                    col,_,val = list(t.keys())[0].split()\n",
    "                    col = int(col)\n",
    "                    try:\n",
    "                        val = float(val)\n",
    "                    except:\n",
    "                        val = str(val)\n",
    "                    key = list(t.keys())[0]\n",
    "                    key_val = t[key]\n",
    "            else:\n",
    "                t = t[key][1]\n",
    "                if type(t) != dict:\n",
    "                    cls.append(t)\n",
    "                    break\n",
    "                else:\n",
    "                    col,_,val = list(t.keys())[0].split()\n",
    "                    col = int(col)\n",
    "                    try:\n",
    "                        val = float(val)\n",
    "                    except:\n",
    "                        val = str(val)\n",
    "                    key = list(t.keys())[0]\n",
    "                    key_val = t[key]\n",
    "    cls = [int(i) for i in cls]\n",
    "    test_data[\"Class\"] = cls\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold(df):\n",
    "    k = int(input(\"Enter k value: \"))\n",
    "    metrics_list = []\n",
    "    for i in range(k):\n",
    "        splitdfs = np.array_split(df, k)\n",
    "        test = splitdfs[i]\n",
    "        del(splitdfs[i])\n",
    "        train = pd.concat(splitdfs)\n",
    "        test.reset_index(inplace = True, drop = True)\n",
    "        train.reset_index(inplace = True, drop = True) \n",
    "        actual = test.iloc[:,-1]\n",
    "        test = test.iloc[:,:-1]\n",
    "        model = decision_tree(train)\n",
    "        results = predict(model, test)\n",
    "        Accuracy, Precision, Recall, f1_score = metrics(actual, results[\"Class\"])\n",
    "        metrics_list.append([Accuracy, Precision, Recall, f1_score])\n",
    "    metrics_list = np.array(metrics_list)\n",
    "    metrics_list = np.mean(metrics_list, axis = 0)\n",
    "    print(\"Accuracy: \",metrics_list[0])\n",
    "    print(\"Precision: \",metrics_list[1])\n",
    "    print(\"Recall: \",metrics_list[2])\n",
    "    print(\"f1_score: \",metrics_list[3])\n",
    "    return metrics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter k value: 5\n",
      "Accuracy:  0.9419034311442323\n",
      "Precision:  0.9080338872450474\n",
      "Recall:  0.9418506280408254\n",
      "f1_score:  0.9232578084172698\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.94190343, 0.90803389, 0.94185063, 0.92325781])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"project3_dataset1.txt\", sep = '\\t', header=None)\n",
    "results1 = k_fold(df1)\n",
    "results1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter k value: 5\n",
      "Accuracy:  0.5799672744273024\n",
      "Precision:  0.4794155844155844\n",
      "Recall:  0.4151728245344544\n",
      "f1_score:  0.4388484949853003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.57996727, 0.47941558, 0.41517282, 0.43884849])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv(\"project3_dataset2.txt\", sep = '\\t', header=None)\n",
    "results2 = k_fold(df2)\n",
    "results2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = pd.read_csv(\"project3_dataset4.txt\", sep = '\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth 1 Impurity measure: 0.3673469387755103\n",
      "Depth 2 Impurity measure: 0.19047619047619047\n",
      "Depth 3 Impurity measure: 0.3333333333333333\n",
      "Depth 4 Impurity measure: 0.0\n",
      "Depth 2 Impurity measure: 0.21428571428571427\n",
      "Depth 3 Impurity measure: 0.0\n",
      "Depth 3 Impurity measure: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'2 <= normal': [{'3 <= weak': [1, {'1 <= mild': [1, {'0 <= rain': [0, 1]}]}]},\n",
       "  {'0 <= rain': [{'3 <= weak': [1, 0]}, {'0 <= sunny': [0, 1]}]}]}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_tree(demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
